{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d30442f2-3d0c-49bb-ae93-3cc6a3c3d913",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from compare_mt.rouge.rouge_scorer import RougeScorer\n",
    "rouge_scorer = RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3b98dd-4676-4695-ba7e-d99a4a083d2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ours'\n",
    "DATASET_NAME = 'xsum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f78312-99fc-43f1-a92d-825ce3944389",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f'./E1-xsum-res-datas/ours-best-gen-xsum-hyps.txt','r') as f:\n",
    "    predictions = f.readlines()\n",
    "    f.close()\n",
    "with open(f'./E1-xsum-res-datas/ours-best-gen-xsum-ref.txt','r') as f:\n",
    "    references = f.readlines()\n",
    "    f.close()\n",
    "score = {'rouge-1':0,'rouge-2':0,'rouge-l':0}\n",
    "n = len(predictions)\n",
    "for  i in range(len(references)):\n",
    "    rouge = rouge_scorer.score(target=references[i],prediction=predictions[i])\n",
    "    score['rouge-1']+= rouge['rouge1'].fmeasure\n",
    "    score['rouge-2']+= rouge['rouge2'].fmeasure\n",
    "    score['rouge-l']+= rouge['rougeLsum'].fmeasure\n",
    "score['rouge-1'] = score['rouge-1'] / n\n",
    "score['rouge-2'] = score['rouge-2'] / n\n",
    "score['rouge-l'] = score['rouge-l'] / n\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741a8c2-6203-4c23-af8b-ed50f91138f3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f'./E1-xsum-res-datas/xsum-2-39100-hyps-48.75.txt','r') as f:\n",
    "    predictions = f.readlines()\n",
    "    f.close()\n",
    "with open(f'./E1-xsum-res-datas/xsum-2-39100-ref48.75.txt','r') as f:\n",
    "    references = f.readlines()\n",
    "    f.close()\n",
    "score = {'rouge-1':0,'rouge-2':0,'rouge-l':0}\n",
    "n = len(predictions)\n",
    "for  i in range(len(references)):\n",
    "    rouge = rouge_scorer.score(target=references[i],prediction=predictions[i])\n",
    "    score['rouge-1']+= rouge['rouge1'].fmeasure\n",
    "    score['rouge-2']+= rouge['rouge2'].fmeasure\n",
    "    score['rouge-l']+= rouge['rougeLsum'].fmeasure\n",
    "score['rouge-1'] = score['rouge-1'] / n\n",
    "score['rouge-2'] = score['rouge-2'] / n\n",
    "score['rouge-l'] = score['rouge-l'] / n\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7cfe4-30c1-4d57-8ddc-abfa43b045f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with open(f'./cnndam-brio-hyps-47.50.txt','r') as f:\n",
    "    predictions = f.readlines()\n",
    "    f.close()\n",
    "with open(f'./cnndam-brio-ref-47.50.txt','r') as f:\n",
    "    references = f.readlines()\n",
    "    f.close()\n",
    "score = {'rouge-1':0,'rouge-2':0,'rouge-l':0}\n",
    "n = len(predictions)\n",
    "for  i in range(len(references)):\n",
    "    rouge = rouge_scorer.score(target=references[i],prediction=predictions[i])\n",
    "    score['rouge-1']+= rouge['rouge1'].fmeasure\n",
    "    score['rouge-2']+= rouge['rouge2'].fmeasure\n",
    "    score['rouge-l']+= rouge['rougeLsum'].fmeasure\n",
    "score['rouge-1'] = score['rouge-1'] / n\n",
    "score['rouge-2'] = score['rouge-2'] / n\n",
    "score['rouge-l'] = score['rouge-l'] / n\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027a0b2e-e5ad-47c8-8852-b432b3d61f53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## BERT_SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8029b-5452-4f5b-8244-13abf1681f0c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\",cache_dir=\"../cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094da9b-8e68-460d-b099-120bd476df7a",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bs = bertscore.compute(predictions=predictions, references=references, lang=\"en\",device = 'cuda',batch_size = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6d5a9-2495-46e9-a59f-e448918365d0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bs_f1 = sum(bs['f1']) / len(bs['f1'])\n",
    "bs_r = sum(bs['recall']) / len(bs['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ee358-2122-4b89-8ce2-c241e68171dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8831419913638042, 'recall': 0.8851193687623226}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'f1':bs_f1,'recall':bs_r}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}